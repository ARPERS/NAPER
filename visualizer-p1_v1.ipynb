{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import torch.multiprocessing as _mp\n",
    "\n",
    "import resnet_cifar10\n",
    "from networks import NetNAPER, NetTMR, NetDMR, NetDRO\n",
    "from utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "import sysconfig\n",
    "print(sysconfig.get_paths()['include'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print('Using device :', device)\n",
    "    print('GPU          :', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    import cpuinfo\n",
    "    cpuinfo.get_cpu_info()['brand']\n",
    "\n",
    "print(\"Pytorch version: \",torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "test_set = datasets.CIFAR10(\"cifar10\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "testloaders = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time = 10\n",
    "\n",
    "model_types = [20,32,44,56]\n",
    "idss =        [[0, 1], [0,1],[0,1],[0,1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_times = []\n",
    "\n",
    "# base\n",
    "for id, mod in zip(idss, model_types):\n",
    "    models = []\n",
    "    ids = id\n",
    "    model_name =\"resnet\"+str(mod)\n",
    "    models_path = \"models/\"\n",
    "\n",
    "    # load all models\n",
    "    models = eval_model_load(str(mod), ids, model_name, models_path, compile=False)\n",
    "    evaluate_acc(models[0], testloaders, single=True, time_profiler=True)\n",
    "    model_time = []\n",
    "    for i in range(run_time):\n",
    "        model_time.append(evaluate_acc(models[0], testloaders, single=True, time_profiler=True)[-1])\n",
    "    base_times.append(model_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meantime = 0.\n",
    "for _ in range(10):\n",
    "    sample_input= next(iter(testloaders))[0][0].unsqueeze(0).to(device)\n",
    "    if device == \"cuda\": \n",
    "        torch.cuda.synchronize()\n",
    "    # with torch.no_grad():\n",
    "    startdmr = time.perf_counter()\n",
    "    models[0](sample_input)\n",
    "    if device == \"cuda\": \n",
    "        torch.cuda.synchronize()\n",
    "    enddmr = time.perf_counter()\n",
    "    meantime += (enddmr-startdmr)*1000\n",
    "meantime /= 10\n",
    "print(\"mean time: \", meantime)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRO_times = []\n",
    "DRO_times2 = []\n",
    "\n",
    "\n",
    "for id, mod in zip(idss, model_types):\n",
    "    models = []\n",
    "    ids = id\n",
    "    model_name =\"resnet\"+str(mod)\n",
    "    models_path = \"models/\"\n",
    "\n",
    "    # load all models\n",
    "    models = eval_model_load(str(mod), ids, model_name, models_path, compile=False)\n",
    "    print(evaluate_acc(models[0], testloaders))\n",
    "\n",
    "    modelDRO = NetDRO(models[0])\n",
    "    modelDRO.to(device)\n",
    "    modelDRO.eval()\n",
    "    print(\"model DRO ready\")\n",
    "    print(\"accuracy\", evaluate_acc(modelDRO, testloaders))\n",
    "\n",
    "    evaluate_acc(modelDRO, testloaders, single=True, time_profiler=True)\n",
    "    print(\"finish warming up..\")\n",
    "    model_time = []\n",
    "    modelDRO.protected=True\n",
    "    \n",
    "    torch.cuda.synchronize() # data masuk\n",
    "    for i in range(run_time):\n",
    "        model_time.append(evaluate_acc(modelDRO, testloaders, single=True, time_profiler=True)[-1])\n",
    "    DRO_times.append(model_time)\n",
    "\n",
    "# save time data\n",
    "with open(\"tmp_p1_mobile_dro.pkl\", \"wb\") as f:\n",
    "    pickle.dump(DRO_times, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMR_times = []\n",
    "TMR_times2 = []\n",
    "\n",
    "for id, mod in zip(idss, model_types):\n",
    "    models = []\n",
    "    ids = id\n",
    "\n",
    "    model_name =\"resnet\"+str(mod)\n",
    "    # model_name =\"mobilenet\"+str(mod)\n",
    "    # mod = \"mobilenet\"\n",
    "\n",
    "    models_path = \"models/\"\n",
    "\n",
    "    # load all models\n",
    "    models = eval_model_load(str(mod), ids, model_name, models_path, compile=False)\n",
    "    print(evaluate_acc(models[0], testloaders))\n",
    "\n",
    "    modelTMR = NetTMR(models[0])\n",
    "    modelTMR.to(device)\n",
    "    modelTMR.eval()\n",
    "    print(\"model TMR ready\")\n",
    "    print(\"accuracy\", evaluate_acc(modelTMR, testloaders))\n",
    "\n",
    "    evaluate_acc(modelTMR, testloaders, single=True, time_profiler=True)\n",
    "    print(\"finish warming up..\")\n",
    "    model_time = []\n",
    "    modelTMR.protected=True\n",
    "    for i in range(run_time):\n",
    "        model_time.append(evaluate_acc(modelTMR, testloaders, single=True, time_profiler=True)[-1])\n",
    "    TMR_times.append(model_time)\n",
    "print(TMR_times)\n",
    "\n",
    "# save time data\n",
    "with open(\"tmp_p1_mobile_tmr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(TMR_times, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBR / DMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMR_times = []\n",
    "DMR_times2 = []\n",
    "\n",
    "for id, mod in zip(idss, model_types):\n",
    "    models = []\n",
    "    ids = id\n",
    "   \n",
    "    model_name =\"resnet\"+str(mod)\n",
    "\n",
    "    models_path = \"models/\"\n",
    "\n",
    "    # load all models\n",
    "    models = eval_model_load(str(mod), ids, model_name, models_path, compile=False)\n",
    "    print(evaluate_acc(models[0], testloaders))\n",
    "\n",
    "    modelDMR = NetDMR(models[0], \"\", sum_model(models[0]))\n",
    "    modelDMR.to(device)\n",
    "    modelDMR.eval()\n",
    "    print(\"model DMR ready\")\n",
    "    print(\"accuracy\", evaluate_acc(modelDMR, testloaders))\n",
    "\n",
    "    evaluate_acc(modelDMR, testloaders, single=True, time_profiler=True)\n",
    "    print(\"finish warming up..\")\n",
    "    model_time = []\n",
    "    modelDMR.protected=True\n",
    "    modelDMR.debug = False\n",
    "    for i in range(run_time):\n",
    "        model_time.append(evaluate_acc(modelDMR, testloaders, single=True, time_profiler=True)[-1])\n",
    "    DMR_times.append(model_time)\n",
    "\n",
    "print(DMR_times)\n",
    "print(\"Average of Total Inference Time (no fault)    :\", \"{:.4f}\".format(np.mean(DMR_times[0])), \"ms\")\n",
    "\n",
    "# save time data\n",
    "with open(\"tmp_p1_mobile_dmr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(DMR_times, f)\n",
    "\n",
    "del modelDMR, models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaPER_times = []\n",
    "NaPER_single = []\n",
    "NaPER_times2 = []\n",
    "\n",
    "# NAPER ENSEMBLE\n",
    "for id, mod in zip(idss, model_types):\n",
    "    models = []\n",
    "    ids = id\n",
    "    \n",
    "    model_name =\"resnet\"+str(mod)\n",
    "    models_path = \"models/\"\n",
    "\n",
    "    # load all models\n",
    "    models = eval_model_load(str(mod), ids, model_name, models_path, compile=False)\n",
    "    print(\"model 0 accuracy\", evaluate_acc(models[0], testloaders))\n",
    "\n",
    "    deltas, summs = get_sum_delta(models)\n",
    "    modelDelta = NetNAPER(models, deltas, summs)\n",
    "    modelDelta.protected=False\n",
    "    print(\"model NaPER ready\")\n",
    "    print(\"ensemble accuracy\",ens_evaluate_acc(wrappermodel=modelDelta, testloaders=testloaders))\n",
    "    print(\"ensemble 0 accuracy\",ens_evaluate_acc(wrappermodel=modelDelta, testloaders=testloaders, limit_model=1))\n",
    "\n",
    "    print(\"finish warming up..\")\n",
    "    model_time = []\n",
    "    modelDelta.protected=True\n",
    "    for i in range(run_time):\n",
    "        model_time.append(ens_evaluate_acc(wrappermodel=modelDelta, testloaders=testloaders, single=True, time_profiler=True)[-1])\n",
    "    NaPER_times.append(model_time)\n",
    "\n",
    "    model_time = []\n",
    "    for i in range(run_time):\n",
    "        model_time.append(ens_evaluate_acc(wrappermodel=modelDelta, testloaders=testloaders, single=True,\n",
    "                                           time_profiler=True, limit_model=1)[-1])\n",
    "    NaPER_single.append(model_time)\n",
    "print(NaPER_times)\n",
    "with open(\"tmp_p1_mobile_naper.pkl\", \"wb\") as f:\n",
    "    pickle.dump(NaPER_times, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsRes_time = []\n",
    "\n",
    "for id, mod in zip(idss, model_types):\n",
    "    models = []\n",
    "    ids = id\n",
    "    model_name =\"resnet\"+str(mod)\n",
    "    models_path = \"models/\"\n",
    "\n",
    "    # load all models\n",
    "    models = eval_model_load(str(mod), ids, model_name, models_path, compile=False)\n",
    "    print(evaluate_acc(models[0], testloaders))\n",
    "\n",
    "    print(\"model ensemble ready\")\n",
    "    print(\"accuracy\",ens_evaluate_acc(models=models, testloaders=testloaders))\n",
    "    ens_evaluate_acc(models=models, testloaders=testloaders, single=True, time_profiler=True)\n",
    "    ens_evaluate_acc(models=models, testloaders=testloaders, single=True, time_profiler=True)\n",
    "    print(\"finish warming up..\")\n",
    "    model_time = []\n",
    "    for i in range(run_time):\n",
    "        model_time.append(ens_evaluate_acc(models=models, testloaders=testloaders, single=True, time_profiler=True)[-1])\n",
    "    EnsRes_time.append(model_time)\n",
    "print(EnsRes_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT - DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACC score are hardcoded for faster visualization, but the model is available in the /models folder\n",
    "\n",
    "        #  20      32      44      56\n",
    "accs_10=[[91.72, 92.31,\t92.92,\t93.25], # resnet accuracy single\n",
    "        [93.26,\t93.82,\t94.02,\t94.65]] # resnet ensemble accuracy\n",
    "\n",
    "        #   20      32      44      56\n",
    "accs_100=[[66.92, 68.82, 70.04,\t70.87], # resnet accuracy single\n",
    "         [71.75, 73.61,\t74.8,\t74.82]] # resnet ensemble accuracy\n",
    "\n",
    "accs_traffic =[[93.00, 93.50, 93.63, 94.74],\n",
    "               [94.89, 95.04, 95.78, 96.02]]\n",
    "\n",
    "acc_eft = [93.9, 74.37, 95.29]\n",
    "\n",
    "cbcol = ['#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT - Overhead No Fault "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    p_base = np.array(base_times[i])\n",
    "    p_tmr =  np.array(TMR_times[i])\n",
    "    tmr_median = np.median(p_tmr)\n",
    "    if i==0:\n",
    "        p_tmr_2 = np.array(TMR_times[i]) / tmr_median\n",
    "        p_naper = np.array(NaPER_times[i])[:,-1] / tmr_median\n",
    "        p_dro =  np.array(DRO_times[i]) / tmr_median\n",
    "        p_cbr =  np.array(DMR_times[i]) / tmr_median\n",
    "    else:\n",
    "        p_tmr_2 *= np.array(TMR_times[i]) / tmr_median\n",
    "        p_naper *= np.array(NaPER_times[i])[:,-1] / tmr_median\n",
    "        p_dro *=  np.array(DRO_times[i]) / tmr_median\n",
    "        p_cbr *=  np.array(DMR_times[i]) / tmr_median\n",
    "p_tmr_2 = np.power(p_tmr_2, 1/4)\n",
    "p_naper = np.power(p_naper, 1/4)\n",
    "p_dro = np.power(p_dro, 1/4)\n",
    "p_cbr = np.power(p_cbr, 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ['NAPER',\n",
    "        #  'ResNet20+NaPER1',\n",
    "         'CBR',\n",
    "         'DRO',\n",
    "         'TMR',]\n",
    "model.reverse()\n",
    "\n",
    "data_all = np.flip(np.array([\n",
    "    p_naper,\n",
    "    p_cbr,\n",
    "    p_dro,\n",
    "    p_tmr_2\n",
    "]))\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(5,2), dpi=200)\n",
    "\n",
    "flierprops = dict(marker='.', markerfacecolor='black', markersize=2, linestyle='none')\n",
    "medianprops = dict(linestyle='--', linewidth=1, color='black')\n",
    "ax.boxplot(data_all.T, vert=True, medianprops=medianprops, flierprops=flierprops)\n",
    "\n",
    "ax.set_xticklabels(model, fontsize=9)\n",
    "ax.tick_params(axis='y', which='major', labelsize=9)\n",
    "ax.set_ylabel('Normalized Inference Time\\n in Normal Condition', fontsize=8.5)\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter(\"{x:.1f}x\"))\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.ylim(ymax=1.03)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT - Convex Hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,2), dpi=200)\n",
    "plt.xlabel(\"Deadline (ms)\", fontsize=9)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=9)\n",
    "plt.xlim(xmax=40)\n",
    "plt.xticks(fontsize=9) \n",
    "plt.yticks(fontsize=9)\n",
    "\n",
    "\n",
    "accs = accs_10\n",
    "\n",
    "xticks = [\"ResNet20\"]\n",
    "def get_mam(arr, naper1=False, naper2=False):    \n",
    "    _avg = np.mean(np.array(arr),axis=1)\n",
    "    _min =  np.min(np.array(arr),axis=1)\n",
    "    _max =  np.max(np.array(arr),axis=1)\n",
    "    return(_avg, _min, _max)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "man_startpoint = 0\n",
    "a,mi,ma = get_mam(base_times)\n",
    "ta1,tmi1,tma1 = get_mam(np.array(EnsRes_time)[:,:,-1])\n",
    "ta = np.concatenate([a[:-1], ta1[man_startpoint:]])\n",
    "tmi = np.concatenate([mi, tmi1[man_startpoint:]])\n",
    "tma = np.concatenate([ma, tma1[man_startpoint:]])\n",
    "accs_here = np.concatenate([accs[0][:-1], accs[1]])\n",
    "plt.step(np.concatenate([ta, [52]]), np.concatenate([accs_here, [accs_here[-1]]]), color=cbcol[4], label=\"Unprotected ResNet\",\n",
    "         linewidth=1, marker=\"P\", markersize=4, linestyle=\"-\", where=\"post\")\n",
    "\n",
    "\n",
    "single_endpoint = 1\n",
    "man_startpoint = -4\n",
    "ta,tmi,tma = get_mam(np.array(NaPER_single)[:,:, 0])\n",
    "ta1,tmi1,tma1 = get_mam(np.array(NaPER_times)[:, :, -1])\n",
    "ta = np.concatenate([ta[:single_endpoint], ta1[man_startpoint:]])\n",
    "tmi = np.concatenate([tmi[:single_endpoint], tmi1[man_startpoint:]])\n",
    "tma = np.concatenate([tma[:single_endpoint], tma1[man_startpoint:]])\n",
    "accs_here = np.concatenate([accs[0][:single_endpoint], accs[1][man_startpoint:]])\n",
    "plt.step(np.concatenate([ta,[50]]), np.concatenate([accs_here,[accs_here[-1]]]),  color=cbcol[0], label=\"NAPER\", linewidth=1, marker=\"o\", markersize=2, where=\"post\")\n",
    "\n",
    "ta,tmi,tma = get_mam(DMR_times)\n",
    "plt.step(np.concatenate([ta,[52]]), np.concatenate([accs[0], [accs[0][-1]]]), color=cbcol[1], label=\"CBR\", linewidth=1, marker=\"D\", markersize=4, where=\"post\")\n",
    "\n",
    "\n",
    "ta,tmi,tma = get_mam(DRO_times)\n",
    "plt.step(np.concatenate([ta,[50]]), np.concatenate([accs[0],[accs[0][-1]]]),  color=cbcol[2], label=\"TMR-MEM\", linewidth=1, marker=\"o\", markersize=2, where=\"post\")\n",
    "\n",
    "\n",
    "ta,tmi,tma = get_mam(TMR_times)\n",
    "plt.step(np.concatenate([ta,[52]]), np.concatenate([accs[0], [accs[0][-1]]]), color=cbcol[3], label=\"TMR\", linewidth=1, marker=\"o\", markersize=4, where=\"post\")\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0.5, -0.3), ncol=5, loc='upper center', fontsize=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT - CIFAR10/CIFAR100 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust figure size to accommodate right-side legend\n",
    "fig, ax = plt.subplots(1, 3, figsize=(7,2.5), dpi=200)\n",
    "fig.subplots_adjust(wspace=0.2)  # Make room for legend on right\n",
    "\n",
    "t = 0\n",
    "\n",
    "### CIFAR100\n",
    "for i in range(3):\n",
    "    ax1 = ax[i]\n",
    "    accs = [accs_10, accs_100, accs_traffic]\n",
    "\n",
    "    # Define the x-values\n",
    "    x = [20, 30, 40, 50] # A6000\n",
    "    # x = [200, 300, 400, 500] # Jetson\n",
    "\n",
    "    # Define the y-values for each category\n",
    "    tmr = [accs[i][0][0], accs[i][0][1], accs[i][0][2], accs[i][0][3]]\n",
    "    cbr = [accs[i][0][1], accs[i][0][2], accs[i][0][3], accs[i][0][3]]\n",
    "    tmrm = [accs[i][0][0], accs[i][0][2], accs[i][0][3], accs[i][0][3]]\n",
    "    unprotected_resnet = [accs[i][1][-1]]*len(x)\n",
    "    naper = [accs[i][1][1],accs[i][1][2],accs[i][1][-1],accs[i][1][-1]]\n",
    "    eft = [acc_eft[i]] * len(x)\n",
    "\n",
    "    # Create the line plots with different markers\n",
    "    ax1.plot(x, unprotected_resnet, marker='x', label='Unprot.\\nResNet', color='black', linestyle='-', markersize=4)\n",
    "    ax1.plot(x, tmr, marker='o', label='TMR', color=cbcol[1], linestyle='-', markersize=4)\n",
    "    ax1.plot(x, cbr, marker='D', label='CBR', color=cbcol[2], linestyle='-', markersize=4)\n",
    "    ax1.plot(x, tmrm, marker='*', label='DRO', color=cbcol[3], linestyle='-', markersize=4)\n",
    "    ax1.plot(x, eft, marker='+', label='EFT', color=cbcol[4], linestyle='-', markersize=6)\n",
    "    ax1.plot(x, naper, marker='^', label='NAPER', color=cbcol[0], linestyle='-', markersize=4)\n",
    "\n",
    "    # Customize the plot\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax1.set_axisbelow(True)\n",
    "    ax1.set_xlabel('Latency Deadline (ms)', fontsize=9)\n",
    "\n",
    "    # Set tick parameters\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=8)  # Smaller tick labels\n",
    "\n",
    "\n",
    "# Set individual y-labels\n",
    "ax[0].set_ylabel('Accuracy. (%)', fontsize=9)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax[0].set_ylim(ymin=91, ymax=95.8)\n",
    "ax[1].set_ylim(ymin=66, ymax=76.2)\n",
    "ax[2].set_ylim(ymin=92, ymax=96.8)\n",
    "\n",
    "ax[0].set_yticks([91, 93, 95])\n",
    "ax[1].set_yticks([66, 70, 75])\n",
    "ax[2].set_yticks([92, 95, 96])\n",
    "\n",
    "# Add dataset titles inside each subplot\n",
    "ax[0].set_title('CIFAR-10', fontsize=8, y=0.9)\n",
    "ax[1].set_title('CIFAR-100', fontsize=8, y=0.9)\n",
    "ax[2].set_title('GTSRB', fontsize=8, y=0.9)\n",
    "\n",
    "# Move legend to bottom\n",
    "plt.legend(bbox_to_anchor=(0.5, -0.15), \n",
    "          loc='center',\n",
    "          ncol=6,  # Three columns\n",
    "          fontsize=8,\n",
    "          bbox_transform=plt.gcf().transFigure)  # Use figure coordinates\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
